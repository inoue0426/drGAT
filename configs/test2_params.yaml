nci:
  cell:
    GAT:
      T_max: 232.0
      activation: gelu
      attention_dropout: 0.0
      dropout1: 0.4
      dropout2: 0.2
      dropout3: 0.1
      epochs: 700.0
      final_mlp_layers: 3.0
      heads: 3.0
      hidden1: 447.0
      hidden2: 73.0
      hidden3: 60.0
      is_zero_pad: true
      lr: 0.0009814391017011538
      n_layers: 3.0
      norm_type: GraphNorm
      optimizer: Adam
      scheduler: Cosine
      weight_decay: 0.0017968445411520497
    GATv2:
      T_max: 415.0
      activation: relu
      attention_dropout: 0.1
      dropout1: 0.1
      dropout2: 0.2
      dropout3: 0.5
      epochs: 1000.0
      final_mlp_layers: 2.0
      heads: 8.0
      hidden1: 258.0
      hidden2: 88.0
      hidden3: 76.0
      is_zero_pad: true
      lr: 0.0003614360902578443
      n_layers: 2.0
      norm_type: GraphNorm
      optimizer: Adam
      scheduler: Cosine
      weight_decay: 2.3409814540170567e-06
    Transformer:
      T_max: 342.0
      activation: gelu
      attention_dropout: 0.0
      dropout1: 0.30000000000000004
      dropout2: 0.30000000000000004
      dropout3: 0.1
      epochs: 1000.0
      final_mlp_layers: 2.0
      heads: 7.0
      hidden1: 257.0
      hidden2: 171.0
      hidden3: 110.0
      is_zero_pad: true
      lr: 0.00019235641624334087
      n_layers: 2.0
      norm_type: BatchNorm
      optimizer: AdamW
      scheduler: Cosine
      weight_decay: 3.163186689146734e-06
  drug:
    GAT:
      T_max: .nan
      activation: relu
      attention_dropout: 0.0
      dropout1: 0.4
      dropout2: 0.4
      dropout3: 0.5
      epochs: 1000.0
      final_mlp_layers: 2.0
      heads: 4.0
      hidden1: 342.0
      hidden2: 202.0
      hidden3: 76.0
      is_zero_pad: true
      lr: 0.0001887948283010459
      n_layers: 3.0
      norm_type: LayerNorm
      optimizer: Adam
      scheduler: null
      weight_decay: 3.5990083992680273e-06
    GATv2:
      T_max: .nan
      activation: relu
      attention_dropout: 0.30000000000000004
      dropout1: 0.4
      dropout2: 0.30000000000000004
      dropout3: 0.5
      epochs: 800.0
      final_mlp_layers: 3.0
      heads: 3.0
      hidden1: 394.0
      hidden2: 152.0
      hidden3: 73.0
      is_zero_pad: true
      lr: 6.804245335072801e-05
      n_layers: 2.0
      norm_type: GraphNorm
      optimizer: AdamW
      scheduler: null
      weight_decay: 4.77875056273996e-06
    Transformer:
      T_max: 323.0
      activation: relu
      attention_dropout: 0.1
      dropout1: 0.30000000000000004
      dropout2: 0.4
      dropout3: 0.5
      epochs: 900.0
      final_mlp_layers: 3.0
      heads: 4.0
      hidden1: 468.0
      hidden2: 207.0
      hidden3: 94.0
      is_zero_pad: true
      lr: 0.0005428988559266772
      n_layers: 3.0
      norm_type: BatchNorm
      optimizer: AdamW
      scheduler: Cosine
      weight_decay: 1.2324491474384186e-05
gdsc1:
  cell:
    GAT:
      T_max: .nan
      activation: gelu
      attention_dropout: 0.30000000000000004
      dropout1: 0.2
      dropout2: 0.2
      dropout3: 0.4
      epochs: 300.0
      final_mlp_layers: 2.0
      heads: 4.0
      hidden1: 356.0
      hidden2: 91.0
      hidden3: 80.0
      is_zero_pad: true
      lr: 8.030017742887186e-05
      n_layers: 2.0
      norm_type: GraphNorm
      optimizer: AdamW
      scheduler: null
      weight_decay: 0.0005162680644670299
    GATv2:
      T_max: 112.0
      activation: gelu
      attention_dropout: 0.0
      dropout1: 0.1
      dropout2: 0.30000000000000004
      dropout3: 0.30000000000000004
      epochs: 300.0
      final_mlp_layers: 2.0
      heads: 4.0
      hidden1: 337.0
      hidden2: 139.0
      hidden3: 60.0
      is_zero_pad: true
      lr: 0.0023312525230609147
      n_layers: 3.0
      norm_type: BatchNorm
      optimizer: Adam
      scheduler: Cosine
      weight_decay: 0.0003935745358425792
    Transformer:
      T_max: .nan
      activation: gelu
      attention_dropout: 0.1
      dropout1: 0.4
      dropout2: 0.4
      dropout3: .nan
      epochs: 600.0
      final_mlp_layers: 1.0
      heads: 2.0
      hidden1: 297.0
      hidden2: 109.0
      hidden3: 69.0
      is_zero_pad: true
      lr: 9.222198257027269e-05
      n_layers: 2.0
      norm_type: GraphNorm
      optimizer: AdamW
      scheduler: null
      weight_decay: 0.0015681078140097654
  drug:
    GAT:
      T_max: 420.0
      activation: gelu
      attention_dropout: 0.0
      dropout1: 0.30000000000000004
      dropout2: 0.1
      dropout3: 0.30000000000000004
      epochs: 1000.0
      final_mlp_layers: 3.0
      heads: 8.0
      hidden1: 449.0
      hidden2: 230.0
      hidden3: 72.0
      is_zero_pad: true
      lr: 1.5539108058701498e-05
      n_layers: 2.0
      norm_type: GraphNorm
      optimizer: Adam
      scheduler: Cosine
      weight_decay: 1.5266610840515545e-05
    GATv2:
      T_max: 180.0
      activation: relu
      attention_dropout: 0.1
      dropout1: 0.1
      dropout2: 0.5
      dropout3: 0.30000000000000004
      epochs: 900.0
      final_mlp_layers: 3.0
      heads: 2.0
      hidden1: 303.0
      hidden2: 244.0
      hidden3: 92.0
      is_zero_pad: true
      lr: 2.7394929088563312e-05
      n_layers: 3.0
      norm_type: LayerNorm
      optimizer: Adam
      scheduler: Cosine
      weight_decay: 0.0018747232887237552
    Transformer:
      T_max: .nan
      activation: relu
      attention_dropout: 0.0
      dropout1: 0.30000000000000004
      dropout2: 0.30000000000000004
      dropout3: .nan
      epochs: 1000.0
      final_mlp_layers: 1.0
      heads: 4.0
      hidden1: 280.0
      hidden2: 155.0
      hidden3: 76.0
      is_zero_pad: true
      lr: 7.695673881576374e-05
      n_layers: 2.0
      norm_type: LayerNorm
      optimizer: AdamW
      scheduler: null
      weight_decay: 5.599711689023603e-06
gdsc2:
  cell:
    GAT:
      T_max: 175.0
      activation: relu
      attention_dropout: 0.0
      dropout1: 0.2
      dropout2: 0.1
      dropout3: .nan
      epochs: 700.0
      final_mlp_layers: 1.0
      heads: 7.0
      hidden1: 262.0
      hidden2: 183.0
      hidden3: 99.0
      is_zero_pad: true
      lr: 0.002819578460901472
      n_layers: 3.0
      norm_type: GraphNorm
      optimizer: AdamW
      scheduler: Cosine
      weight_decay: 5.058984853832252e-05
    GATv2:
      T_max: .nan
      activation: gelu
      attention_dropout: 0.1
      dropout1: 0.4
      dropout2: 0.4
      dropout3: .nan
      epochs: 1000.0
      final_mlp_layers: 1.0
      heads: 2.0
      hidden1: 469.0
      hidden2: 190.0
      hidden3: 86.0
      is_zero_pad: false
      lr: 0.00038592153232035
      n_layers: 3.0
      norm_type: GraphNorm
      optimizer: AdamW
      scheduler: null
      weight_decay: 0.00010641518361155301
    Transformer:
      T_max: .nan
      activation: gelu
      attention_dropout: 0.0
      dropout1: 0.4
      dropout2: 0.1
      dropout3: .nan
      epochs: 800.0
      final_mlp_layers: 1.0
      heads: 2.0
      hidden1: 314.0
      hidden2: 67.0
      hidden3: 42.0
      is_zero_pad: false
      lr: 0.0007758590072466671
      n_layers: 3.0
      norm_type: GraphNorm
      optimizer: AdamW
      scheduler: null
      weight_decay: 0.0003425060924475356
  drug:
    GAT:
      T_max: 380.0
      activation: relu
      attention_dropout: 0.0
      dropout1: 0.1
      dropout2: 0.4
      dropout3: 0.4
      epochs: 1000.0
      final_mlp_layers: 2.0
      heads: 4.0
      hidden1: 419.0
      hidden2: 144.0
      hidden3: 125.0
      is_zero_pad: false
      lr: 1.980752765828618e-05
      n_layers: 2.0
      norm_type: BatchNorm
      optimizer: AdamW
      scheduler: Cosine
      weight_decay: 0.00035175309762854983
    GATv2:
      T_max: .nan
      activation: gelu
      attention_dropout: 0.2
      dropout1: 0.1
      dropout2: 0.4
      dropout3: 0.1
      epochs: 400.0
      final_mlp_layers: 2.0
      heads: 8.0
      hidden1: 472.0
      hidden2: 235.0
      hidden3: 35.0
      is_zero_pad: true
      lr: 8.483959383659438e-05
      n_layers: 2.0
      norm_type: BatchNorm
      optimizer: AdamW
      scheduler: null
      weight_decay: 1.1818960547889818e-05
    Transformer:
      T_max: .nan
      activation: relu
      attention_dropout: 0.30000000000000004
      dropout1: 0.1
      dropout2: 0.30000000000000004
      dropout3: 0.1
      epochs: 400.0
      final_mlp_layers: 2.0
      heads: 5.0
      hidden1: 419.0
      hidden2: 238.0
      hidden3: 120.0
      is_zero_pad: true
      lr: 8.532221486478315e-05
      n_layers: 2.0
      norm_type: BatchNorm
      optimizer: AdamW
      scheduler: null
      weight_decay: 3.4323344791383687e-06
ctrp:
  cell:
    GAT:
      T_max: 317.0
      activation: gelu
      attention_dropout: 0.30000000000000004
      dropout1: 0.5
      dropout2: 0.5
      dropout3: 0.5
      epochs: 700.0
      final_mlp_layers: 2.0
      heads: 8.0
      hidden1: 271.0
      hidden2: 147.0
      hidden3: 128.0
      is_zero_pad: true
      lr: 2.1038792700111038e-05
      n_layers: 2.0
      norm_type: LayerNorm
      optimizer: AdamW
      scheduler: Cosine
      weight_decay: 3.586843291091611e-05
    GATv2:
      T_max: .nan
      activation: gelu
      attention_dropout: 0.30000000000000004
      dropout1: 0.30000000000000004
      dropout2: 0.2
      dropout3: .nan
      epochs: 800.0
      final_mlp_layers: 1.0
      heads: 2.0
      hidden1: 482.0
      hidden2: 68.0
      hidden3: 36.0
      is_zero_pad: true
      lr: 2.459895686073573e-05
      n_layers: 2.0
      norm_type: GraphNorm
      optimizer: Adam
      scheduler: null
      weight_decay: 5.3810305791612865e-05
    Transformer:
      T_max: .nan
      activation: relu
      attention_dropout: 0.0
      dropout1: 0.30000000000000004
      dropout2: 0.30000000000000004
      dropout3: 0.5
      epochs: 900.0
      final_mlp_layers: 3.0
      heads: 3.0
      hidden1: 295.0
      hidden2: 104.0
      hidden3: 81.0
      is_zero_pad: true
      lr: 7.104109130955182e-05
      n_layers: 2.0
      norm_type: GraphNorm
      optimizer: AdamW
      scheduler: null
      weight_decay: 2.0659446333939855e-06
  drug:
    GAT:
      T_max: .nan
      activation: gelu
      attention_dropout: 0.0
      dropout1: 0.4
      dropout2: 0.2
      dropout3: 0.30000000000000004
      epochs: 700.0
      final_mlp_layers: 3.0
      heads: 5.0
      hidden1: 499.0
      hidden2: 152.0
      hidden3: 88.0
      is_zero_pad: true
      lr: 0.0012063300892914206
      n_layers: 4.0
      norm_type: GraphNorm
      optimizer: Adam
      scheduler: null
      weight_decay: 0.0001110239844411032
    GATv2:
      T_max: .nan
      activation: relu
      attention_dropout: 0.4
      dropout1: 0.5
      dropout2: 0.5
      dropout3: 0.2
      epochs: 900.0
      final_mlp_layers: 3.0
      heads: 4.0
      hidden1: 414.0
      hidden2: 175.0
      hidden3: 34.0
      is_zero_pad: true
      lr: 0.00017173474157930298
      n_layers: 2.0
      norm_type: GraphNorm
      optimizer: Adam
      scheduler: null
      weight_decay: 0.0012195727357749327
    Transformer:
      T_max: 183.0
      activation: relu
      attention_dropout: 0.2
      dropout1: 0.2
      dropout2: 0.2
      dropout3: 0.2
      epochs: 600.0
      final_mlp_layers: 2.0
      heads: 3.0
      hidden1: 421.0
      hidden2: 120.0
      hidden3: 47.0
      is_zero_pad: true
      lr: 0.00010970329600928436
      n_layers: 3.0
      norm_type: GraphNorm
      optimizer: Adam
      scheduler: Cosine
      weight_decay: 0.001773507293085185
