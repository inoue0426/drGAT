ctrp:
  GAT:
    activation: gelu
    attention_dropout: 0.0
    dropout1: 0.1
    dropout2: 0.2
    dropout3: 0.5
    epochs: 800
    final_mlp_layers: 3
    heads: 6
    hidden1: 398
    hidden2: 83
    hidden3: 54
    is_zero_pad: true
    lr: 0.0008575568077138319
    n_layers: 2
    norm_type: GraphNorm
    optimizer: Adam
    weight_decay: 2.793149007230527e-06
    gnn_layer: GAT
    scheduler: None
  GATv2:
    activation: gelu
    attention_dropout: 0.0
    dropout1: 0.1
    dropout2: 0.2
    dropout3: 0.5
    epochs: 1000
    final_mlp_layers: 2
    heads: 4
    hidden1: 436
    hidden2: 93
    hidden3: 47
    is_zero_pad: true
    lr: 0.001936411075355028
    n_layers: 3
    norm_type: GraphNorm
    optimizer: AdamW
    weight_decay: 2.2267039384631575e-06
    gnn_layer: GATv2
    scheduler: None
  Transformer:
    activation: gelu
    attention_dropout: 0.0
    dropout1: 0.1
    dropout2: 0.1
    dropout3: 0.5
    epochs: 1000
    final_mlp_layers: 2
    heads: 3
    hidden1: 368
    hidden2: 153
    hidden3: 53
    is_zero_pad: true
    lr: 0.0007548912398014524
    n_layers: 3
    norm_type: GraphNorm
    optimizer: AdamW
    weight_decay: 0.003484242253833941
    gnn_layer: Transformer
    scheduler: None
gdsc1:
  GAT:
    activation: gelu
    attention_dropout: 0.0
    dropout1: 0.1
    dropout2: 0.1
    dropout3: 0.4
    epochs: 800
    final_mlp_layers: 2
    heads: 7
    hidden1: 325
    hidden2: 236
    hidden3: 38
    is_zero_pad: true
    lr: 0.0007016719188357202
    n_layers: 4
    norm_type: GraphNorm
    optimizer: AdamW
    weight_decay: 0.00018548045219984565
    gnn_layer: GAT
    scheduler: None
  GATv2:
    activation: gelu
    attention_dropout: 0.0
    dropout1: 0.1
    dropout2: 0.4
    dropout3: 0.5
    epochs: 1000
    final_mlp_layers: 2
    heads: 6
    hidden1: 400
    hidden2: 85
    hidden3: 46
    is_zero_pad: true
    lr: 0.00293126191608855
    n_layers: 2
    norm_type: GraphNorm
    optimizer: AdamW
    weight_decay: 0.0010438524905917045
    gnn_layer: GATv2
    scheduler: None
  Transformer:
    activation: gelu
    attention_dropout: 0.1
    dropout1: 0.1
    dropout2: 0.30000000000000004
    dropout3: 0.5
    epochs: 1000
    final_mlp_layers: 2
    heads: 6
    hidden1: 283
    hidden2: 134
    hidden3: 85
    is_zero_pad: true
    lr: 0.0004947944343045193
    n_layers: 3
    norm_type: GraphNorm
    optimizer: Adam
    weight_decay: 3.6764350514067637e-06
    gnn_layer: Transformer
    scheduler: None
gdsc2:
  GAT:
    activation: gelu
    attention_dropout: 0.0
    dropout1: 0.1
    dropout2: 0.2
    dropout3: 0.5
    epochs: 1000
    final_mlp_layers: 2
    heads: 7
    hidden1: 496
    hidden2: 72
    hidden3: 69
    is_zero_pad: false
    lr: 0.0005308772677381622
    n_layers: 2
    norm_type: GraphNorm
    optimizer: Adam
    weight_decay: 8.791339734457234e-06
    gnn_layer: GAT
    scheduler: None
  GATv2:
    activation: relu
    attention_dropout: 0.0
    dropout1: 0.1
    dropout2: 0.1
    dropout3: 0.5
    epochs: 900
    final_mlp_layers: 2
    heads: 3
    hidden1: 377
    hidden2: 103
    hidden3: 79
    is_zero_pad: false
    lr: 0.0038807538692928736
    n_layers: 2
    norm_type: GraphNorm
    optimizer: AdamW
    weight_decay: 1.9640326788268122e-05
    gnn_layer: GATv2
    scheduler: None
  Transformer:
    activation: relu
    attention_dropout: 0.2
    dropout1: 0.5
    dropout2: 0.1
    dropout3: 0.5
    epochs: 1000
    final_mlp_layers: 2
    heads: 6
    hidden1: 485
    hidden2: 174
    hidden3: 64
    is_zero_pad: true
    lr: 0.00019491664258032877
    n_layers: 3
    norm_type: GraphNorm
    optimizer: AdamW
    weight_decay: 2.691112977149023e-05
    gnn_layer: Transformer
    scheduler: None
nci:
  GAT:
    activation: relu
    attention_dropout: 0.2
    dropout1: 0.1
    dropout2: 0.1
    dropout3: 0.5
    epochs: 1000
    final_mlp_layers: 2
    heads: 7
    hidden1: 259
    hidden2: 96
    hidden3: 91
    is_zero_pad: true
    lr: 0.00026524367608106615
    n_layers: 2
    norm_type: LayerNorm
    optimizer: Adam
    weight_decay: 0.0006045120590902108
    gnn_layer: GAT
    scheduler: None
  GATv2:
    activation: relu
    attention_dropout: 0.2
    dropout1: 0.2
    dropout2: 0.1
    dropout3: 0.5
    epochs: 1000
    final_mlp_layers: 2
    heads: 6
    hidden1: 395
    hidden2: 236
    hidden3: 80
    is_zero_pad: true
    lr: 0.000376316082193506
    n_layers: 2
    norm_type: LayerNorm
    optimizer: Adam
    weight_decay: 0.0008408712872643064
    gnn_layer: GATv2
    scheduler: None
  Transformer:
    activation: relu
    attention_dropout: 0.1
    dropout1: 0.1
    dropout2: 0.2
    dropout3: 0.5
    epochs: 1000
    final_mlp_layers: 2
    heads: 8
    hidden1: 289
    hidden2: 215
    hidden3: 66
    is_zero_pad: true
    lr: 0.00011219837311484963
    n_layers: 2
    norm_type: GraphNorm
    optimizer: Adam
    weight_decay: 0.0023400993222972882
    gnn_layer: Transformer
    scheduler: None
